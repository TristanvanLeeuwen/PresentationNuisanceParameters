{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimating nuisance parameters in inverse problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on \n",
    "\n",
    "* [Estimating nuisance parameters in inverse problems](https://iopscience.iop.org/article/10.1088/0266-5611/28/11/115016).\n",
    "A.Y. Aravkin and T. van Leeuwen. Inverse Problems (28:11)\n",
    "\n",
    "* [Variable Projection for NonSmooth Problems](https://epubs.siam.org/doi/abs/10.1137/20M1348650). T. van Leeuwen and A.Y. Aravkin. SIAM J. Sci. Comput. (accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "(Motivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "* General formulation\n",
    "* Variable projection\n",
    "* Examples\n",
    "* Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "MAP estimation for inverse problems can be generally formulated as \n",
    "\n",
    "$$\\min_u F(u) + G(u),$$\n",
    "\n",
    "with $F(u) = -\\log \\pi_{\\text{data}}(u)$ and  $G(u) = -\\log \\pi_{\\text{prior}}(u)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The densities often include parameters $\\theta$ (mean, variance, ...). This leads to \n",
    "\n",
    "$$\\min_{u,\\theta} F(u,\\theta) + G(u,\\theta) + H(\\theta),$$\n",
    "\n",
    "with $H(\\theta) = -\\log \\pi_{\\text{hyper}}(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> **Example: *Extended least-squares:***\n",
    "> $$\\min_{u,\\Sigma} m \\log |\\Sigma| + \\sum_{i=1}^m \\|K(u) - f^\\delta\\|_{\\Sigma^{-1}}^2.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> **Example: *Robust data-fitting:***\n",
    "> $$\\min_{u,\\sigma^2} m\\log\\sigma + \\sum_{i=1}^m \\log\\left(1 + \\frac{r_i(u)^2}{\\sigma^2}\\right), \\quad r(u) = K(u) - f.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example: *sparse regularisation:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminate $u$ and formulate\n",
    "\n",
    "$$\\min_{\\theta} \\left(\\min_u F(u,\\theta) + G(u,\\theta) + H(\\theta)\\right).$$\n",
    "\n",
    "* Evaluation for given $\\theta$ requires solution of inverse problem\n",
    "* Results in non-linear problem in $\\theta$ with expensive function evaluations\n",
    "* Can possibly use surrogate modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variable projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Often, the nuisance parameters separate, and we can write\n",
    "\n",
    "$$\\min_u \\left(\\underbrace{\\min_\\alpha F(u,\\alpha)}_{\\overline{F}(u)} + \\underbrace{\\min_\\beta G(u,\\beta)}_{\\overline{G}(u)}\\right).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and apply a proximal-gradient algorithm\n",
    "\n",
    "$$u_{k+1} = \\text{prox}_{\\lambda \\overline{G}} \\left(u_k - \\lambda \\nabla \\overline{F}(u)\\right).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Derivative formula\n",
    "Assuming that $\\nabla_u F$ and $\\nabla_\\alpha F$ are Lipschitz-continuous and $F$ is $\\mu-$ strongly convex in $\\alpha$ for all $u$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* $\\overline{\\alpha}(u) = \\text{arg}\\min_\\alpha F(u,\\alpha)$ is Lipschitz continuous \n",
    "* $\\nabla \\overline{F}(u) = \\left.\\nabla_u F(u,\\alpha)\\right|_{\\alpha = \\overline{\\alpha}(u)}.$\n",
    "* An approximate minimiser $\\widetilde{\\alpha}(u)$ yields an approximate gradient with\n",
    "\n",
    "$$\\|\\nabla F(u,\\overline{\\alpha}(u)) - \\nabla F(u,\\widetilde{\\alpha}(u))\\| \\leq L_{u\\alpha} \\|\\overline{\\alpha}(u) -\\widetilde{\\alpha}(u)\\|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Proximal operator\n",
    "\n",
    "$$\\text{prox}_{\\lambda \\overline{G}}\\left(v\\right) = \\text{arg}\\min_{u,\\beta} \\textstyle{\\frac{1}{2}}\\|u - v\\|_2^2 + G(u,\\beta).$$\n",
    "\n",
    "* Still a proximal operator as long as $\\overline{G}$ is convex\n",
    "* Solution may be available in closed-form\n",
    "* Approximate solution is ok, as long as $\\min_u \\textstyle{\\frac{1}{2}}\\|u - v\\|_2^2 + G(u,\\widetilde{\\beta}(u)) \\leq \\delta + \\min_{u} \\textstyle{\\frac{1}{2}}\\|u - v\\|_2^2 + \\overline{G}(u).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convergence results\n",
    "\n",
    "Convergence of proximal gradient with inexact gradients ($\\epsilon_k$) and inexact proximal operators ($\\delta_k$)[^1]:\n",
    "\n",
    "* sublinear ($\\mathcal{O}(1/k)$) convergence rate for *convex* problems if $\\{\\epsilon_k\\}$ and $\\{\\delta_k\\}$ are summable\n",
    "* linear convergenge if $\\{\\epsilon_k\\}$ and $\\{\\delta_k\\}$ decrease linearly\n",
    "\n",
    "[^1]:Schmidt, Mark, Nicolas Le Roux, and Francis Bach. \"Convergence rates of inexact proximal-gradient methods for convex optimization.\" NeurIps (2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "* Closed-form solution may be available\n",
    "* Can be solved to desired accuracy when $\\theta$ represents only a few variables\n",
    "* Able to re-use existing code for gradient computations and optimisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tikhonov regularisation\n",
    "\n",
    "Given $K \\in \\mathbb{R}^{m\\times n}$ and $f^\\delta = Ku_{\\text{true}} + \\epsilon$ with $\\epsilon \\sim N(0,\\sigma^2)$, solve\n",
    "\n",
    "$$\\min_{u,\\alpha,\\beta} \\alpha \\|Ku - f^\\delta\\|_2^2 + \\beta \\|Lu\\|_2^2 - m \\log \\alpha - n\\log\\beta.$$\n",
    "\n",
    "* $\\overline{\\alpha}(u) = m\\|Ku - f^\\delta\\|_2^{-2}$, $\\overline{\\beta}(u) = n\\|Lu\\|_2^{-2}$ (solve for $u$ with BFGS)\n",
    "* $\\overline{u}(\\alpha,\\beta) = \\left(K^*\\!K + (\\beta/\\alpha)L^*\\!L\\right)^{-1}K^*f^\\delta$ (solve for $(\\alpha,\\beta)$ with Nelder-Mead)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./figures/Tikh_u_hat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./figures/Tikh_theta_hat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](figures/Tikh_convergence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Covariance estimation\n",
    "\n",
    "Non-linear seismic inversion of multi-experiment data.\n",
    "\n",
    "* $F(u,\\Sigma) = m\\log|\\Sigma| + \\sum_{i=1}^m\\|K(u)q_i - f_i^\\delta\\|_{\\Sigma^{-1}}^2.$\n",
    "* Use sample-covariance at each iteration\n",
    "* Can use further tricks to get low-rank approximation of $\\Sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./figures/experiment5_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./figures/experiment4_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Robust fitting^[1]\n",
    "\n",
    "Tomograpraphic image reconstruction from data with outliers:\n",
    "\n",
    "* $F(u,\\alpha) = \\sum_{i=1}^m \\log\\left(1 + \\alpha(Ku-f^\\delta)_i^2\\right) -2m\\log(\\alpha).$\n",
    "* $G(u) = \\text{TV}(u)$\n",
    "* Solve for $\\alpha$ using scalar minimisation method\n",
    "\n",
    "[^1]:Kazantsev, D., Bleichrodt, F., Van Leeuwen, T., Kaestner, A., Withers, P., Batenburg, K. J., & Lee, P. (2017). A novel tomographic reconstruction method based on the robust Student’s t function for suppressing data outliers. IEEE Transactions on Computational Imaging, 1–1. https://doi.org/10.1109/TCI.2017.2694607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./figures/studentst1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrap-up\n",
    "\n",
    "* Algorithmic framework for estimating hyper parameters in inverse problems\n",
    "* ...\n",
    "\n",
    "Other relevant issues:\n",
    "\n",
    "* Marginalisation of $\\theta$\n",
    "* Choice of Hyper-prior\n",
    "* Regularising properties and convergence rates (as noise vanishes)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}